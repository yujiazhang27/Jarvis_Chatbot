{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> The Evaluation of the Chatbot *Jarvis*</h1></center>\n",
    "<br>\n",
    "<center>Yujia Zhang<br>\n",
    "STAT/CS 287 Data Science I, Fall 2017<br>\n",
    "Instructor: James Bagrow<br>\n",
    "*College of Engineering and Mathematical Sciences, UVM*</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import sys, os\n",
    "import sqlite3\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "def findMisPredicted(predicted):\n",
    "    \"\"\"Find which text was wrongly predicted by current model.\n",
    "    show that text, predicted lable and correct lable.\n",
    "    \"\"\"\n",
    "    for i in range(len(lable)):\n",
    "        if predicted[i] != lable[i]:\n",
    "            for row in c.execute(\"SELECT * from training_data\"):\n",
    "                if row[0] == i:\n",
    "                    print(\"'{}' was labled as '{}' but was predicted as '{}'.\".format(row[1],lable[i],predicted[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"files/taining.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "Jarvis is online chatbot designed and trained on Slack, an online communication platform. Jarvis was trained by a corpus consisting of many documents for each label. With training inputs were stored in a database, Jarvis will work as a text classifier: users send it a chat message and it will use a text classification algorithm to decide the class of the documents. <br>\n",
    "<center>**Example of Jarvis in Training Mode**\n",
    "![training.png](training.png)</center>\n",
    "<br>\n",
    "<center>**Example of Jarvis in Testing Mode**\n",
    "![testing.png](testing.png)</center>\n",
    "<br>\n",
    "In this report, two scikit-learn classifers will be evaluated about based on their performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation\n",
    "Two trained classifiers were persisted using Python’s built-in persistence model, namely pickle for future use without retraining. In this evaluation, two classifiers will be loaded separately, then make predictions using training database, and compare the predicted label with the original label to determine the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database that contains all training text and lables\n",
    "db_file = \"jarvis.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Extract training text and lables from database\n",
    "text = []\n",
    "lable = []\n",
    "for row in c.execute(\"SELECT * from training_data\"):\n",
    "    text.append(row[1])\n",
    "    lable.append(row[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Multinomial Naïve Bayes* Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'long time no see' was labled as 'GREETING' but was predicted as 'WEATHER'.\n",
      "'tell me a joke' was labled as 'JOKE' but was predicted as 'GREETING'.\n",
      "'find the best joke and tell me' was labled as 'JOKE' but was predicted as 'WEATHER'.\n",
      "\n",
      "The estimated NB classification accuracy is 0.9286.\n"
     ]
    }
   ],
   "source": [
    "# Load trained NB model\n",
    "if os.path.getsize('jarvis_brain.pkl') > 0: \n",
    "    model_NB = pickle.load(open('jarvis_brain.pkl', 'rb'))\n",
    "\n",
    "# Predict ACTION using training text by cross-validation givin current model\n",
    "predicted = cross_val_predict(model_NB, text, lable, cv=5)\n",
    "# Show those text which predicted result does not match with the original lable\n",
    "findMisPredicted(predicted)\n",
    "\n",
    "# Calculate the accuracy of this prediction\n",
    "accuracy = metrics.accuracy_score(lable, predicted)\n",
    "print()\n",
    "print(\"The estimated NB classification accuracy is {0:.4f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I measured this accuracy by cross-validation using training text. Since we have five classes, we used a k-fold (k=5) cross validations here, which means the training text set is split into 5 smaller sets. For each of the 5 \"folds\", the model is trained using 4 of the folds as training data. Then the model is validated on the remaining fold. The overall accuracy is computed by repeating k-fold cross-validation and averaging the values in each loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   GREETING       0.90      0.90      0.90        10\n",
      "       JOKE       1.00      0.75      0.86         8\n",
      "      PIZZA       1.00      1.00      1.00         8\n",
      "       TIME       1.00      1.00      1.00         8\n",
      "    WEATHER       0.80      1.00      0.89         8\n",
      "\n",
      "avg / total       0.94      0.93      0.93        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = metrics.classification_report(lable, predicted)\n",
    "print(accuracy_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed analysis of computed accuracy using cross-validation can be found in the metric above. \n",
    "\n",
    "For each label, the \"precision\" column is calculated by dividing the number of correct prediction by the total number of returned results. \"recall\" which is also known as sensitivity is calculated by dividing the number of correct prediction by the total number of prediction. \"recall\" is the relatively more important score in accuracy measuring and determining how good our model is. \"f1-score\" which is also known as \"f-measure\" shows the average of \"precision\" and \"recall\" of each label.\n",
    "\n",
    "From the predicting results that we showed in the previous cell, two texts labeled \"JOKE\" were mispredicted and one text labeled \"GREETING\" was mispredicted. So it makes sense that \"JOKE\" has the lowest recall score and \"GREETING\" has the second lowest score. Two texts were mispredicted as \"WEATHER\" and one text was mispredicted as \"GREETING\" which is also corresponded with the fact that \"WEATHER\" has the lowest precision and \"GREETING\" has the second lowest precision.\n",
    "\n",
    "From my personal perspective, I think those texts containing \"find\", \"see\" and \"tell\" are easy to be mispredicted, because multiple labels can be related to those keywords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Support Vector Machines (SVM)* classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'how are you doing' was labled as 'GREETING' but was predicted as 'JOKE'.\n",
      "'long time no see' was labled as 'GREETING' but was predicted as 'JOKE'.\n",
      "'find the best joke and tell me' was labled as 'JOKE' but was predicted as 'WEATHER'.\n",
      "\n",
      "The estimated SVM classification accuracy is 0.9286.\n"
     ]
    }
   ],
   "source": [
    "# Load trained SVM model\n",
    "if os.path.getsize('jarvis_brain_svm.pkl') > 0: \n",
    "    model_SVM = pickle.load(open('jarvis_brain_svm.pkl', 'rb'))\n",
    "\n",
    "# Predict ACTION using training text by cross-validation givin current model\n",
    "predicted_svm = cross_val_predict(model_SVM, text, lable, cv=5)\n",
    "\n",
    "# Show those text which predicted result does not match with the original lable\n",
    "findMisPredicted(predicted_svm)\n",
    "\n",
    "# Calculate the accuracy of this prediction\n",
    "accuracy_svm = metrics.accuracy_score(lable, predicted_svm)\n",
    "print()\n",
    "print(\"The estimated SVM classification accuracy is {0:.4f}.\".format(accuracy_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   GREETING       1.00      0.80      0.89        10\n",
      "       JOKE       0.78      0.88      0.82         8\n",
      "      PIZZA       1.00      1.00      1.00         8\n",
      "       TIME       1.00      1.00      1.00         8\n",
      "    WEATHER       0.89      1.00      0.94         8\n",
      "\n",
      "avg / total       0.94      0.93      0.93        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric_svm = metrics.classification_report(lable, predicted_svm)\n",
    "print(accuracy_metric_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SVM classifier, each data item is plotted as a point in 2-dimensional space with the value of each feature being the value of a particular coordinate. Then the classification is performed by finding the hyperplane that differentiates the classes well. Support Vectors are simply the coordinates of individual observation. It is a frontier which best segregates the classes. Even though text data are studied here, those data can be transformed into the numerical form based on their labels. So SVM classifier can still perform well here.\n",
    "\n",
    "The performance of the SVM classifier heavily depends on the support vectors. There are multiple options and parameters set for the support vectors. The linear support vectors were used here because it's simple to use and understand. Based on the accuracy matrix above, this SVM classifier works very well with this dataset. The accuracy of its prediction is 0.92.86, roughly has the same accuracy as the Naive Bayes classifier does. However, the support vectors are not consistent, so sometimes, the accuracy may get lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Discussion\n",
    "\n",
    "Both classifiers have their pros and cons. Naive Bayes classifier uses \"bag of words\" concept, and make the prediction based on the calculated the posterior probability. It is easy to understand and works well with categorical data. But if there was not observed in training data set, then the model will assign a 0 probability and will be unable to make a prediction about the label. Also \"independent predictors\" assumption of the Naive Bayes is almost impossible to be true in the real life. \n",
    "\n",
    "On the other hand, SVM classifier makes predictions based on the support vectors that segmented the original data. It works well with data that has clear margins and is more flexible since there are multiple support vectors available (linear, polynomial, and etc.). But it does not perform very well with data with more noise, which could be the reason for inconsistent accuracy in this Jarvis training data.\n",
    "\n",
    "I think phasing same meaning in different ways is very helpful with recognizing natural language. One benefits of this are it can help with filtering out those less important words which appear in most of the classes and make those \"real\" keywords weighted more in prediction. Also increasing the number of input can be helpful too. With more input, the variation between each class can be bigger and the variation within each class will be smaller. So the prediction would be more accurate.\n",
    "\n",
    "To increase the computed accuracy of prediction, having the relatively same number of input for each class is helpful too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
